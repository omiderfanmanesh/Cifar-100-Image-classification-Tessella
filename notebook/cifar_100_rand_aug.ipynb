{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from time import time\n",
    "\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from config import cfg\n",
    "from utils import AverageMeter\n",
    "from utils import utilities\n",
    "\n",
    "SEED = 2021\n",
    "random.seed(SEED)\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.empty_cache()\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(SEED)\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from data.dataset import CIFAR100\n",
    "\n",
    "batch_size = cfg.SOLVER.IMS_PER_BATCH\n",
    "num_workers = cfg.DATALOADER.NUM_WORKERS\n",
    "from RandAugment import RandAugment"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "normalize_transform = T.Normalize(mean=(0.5071, 0.4865, 0.4409),\n",
    "                                  std=(0.2673, 0.2564, 0.2762))\n",
    "\n",
    "transformer = T.Compose([\n",
    "    # T.RandomCrop(32, padding=4, padding_mode='reflect'),\n",
    "    # T.RandomHorizontalFlip(),\n",
    "    RandAugment(n=2, m=9),\n",
    "    T.ToTensor(),\n",
    "    normalize_transform\n",
    "])\n",
    "val_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    normalize_transform\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CIFAR100(\n",
    "    root='./data/dataset/cifar_100_dataset_file', train=True,\n",
    "    download=True, transform=transformer,\n",
    ")\n",
    "\n",
    "validation_dataset = CIFAR100(\n",
    "    root='./data/dataset/cifar_100_dataset_file', train=False,\n",
    "    download=True, transform=val_transform,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, num_workers=num_workers\n",
    ")\n",
    "\n",
    "val_data_loader = DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size=cfg.TEST.IMS_PER_BATCH,\n",
    "    num_workers=num_workers\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n",
    "def train(\n",
    "        cfg,\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        criterion,\n",
    "        grad_clip=None,\n",
    "        epochs=50\n",
    "):\n",
    "    model_name = cfg.MODEL.NAME\n",
    "    log_period = cfg.SOLVER.LOG_PERIOD\n",
    "    checkpoint_period = cfg.SOLVER.CHECKPOINT_PERIOD\n",
    "    output_dir = cfg.DIR.OUTPUT_DIR\n",
    "    device = cfg.MODEL.DEVICE\n",
    "    device = torch.device(device=device)\n",
    "\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    min_valid_loss = np.inf\n",
    "\n",
    "    for e in range(epochs):\n",
    "        batch_time = AverageMeter('Time', ':6.3f')\n",
    "        data_time = AverageMeter('Data', ':6.3f')\n",
    "        train_losses = AverageMeter('Training Loss', ':.4e')\n",
    "        val_losses = AverageMeter('Validation Loss', ':.4e')\n",
    "        train_accuracy = AverageMeter('Training Accuracy', ':6.2f')\n",
    "        val_accuracy = AverageMeter('Validation Accuracy', ':6.2f')\n",
    "        train_f1 = AverageMeter('Training F1 score', ':6.2f')\n",
    "        val_f1 = AverageMeter('Validation F1 score', ':6.2f')\n",
    "\n",
    "        start_epoch = time()\n",
    "        model.train()\n",
    "        for itr, (data, labels) in enumerate(train_loader):\n",
    "            start_batch = time()\n",
    "            # Transfer Data to GPU if available\n",
    "            if torch.cuda.is_available():\n",
    "                data, labels = data.cuda(), labels.cuda()\n",
    "\n",
    "\n",
    "            # Forward Pass\n",
    "            target = model(data)\n",
    "            # Find the Loss\n",
    "            loss = criterion(target, labels)\n",
    "            # Calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "             # Gradient clipping\n",
    "            if grad_clip:\n",
    "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "            # Update Weights\n",
    "            optimizer.step()\n",
    "              # Clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "            # Calculate Loss\n",
    "            # train_loss = loss.item() * data.size(0)\n",
    "            # accuracy\n",
    "            acc = utilities.accuracy(y_true=labels, y_pred=target)\n",
    "            # print(acc)\n",
    "            _, predicted = torch.max(target.data, 1)\n",
    "            # f = f1_score(labels.cpu(), predicted.cpu(), average='micro')\n",
    "            # print(f)\n",
    "            # train_f1.update(f)\n",
    "            train_accuracy.update(acc)\n",
    "            train_losses.update(loss.item(), data.size(0))\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time() - start_batch)\n",
    "\n",
    "            utilities.progress_bar(current=itr, total=len(train_loader))\n",
    "\n",
    "        if val_loader:\n",
    "            model.eval()  # Optional when not using Model Specific layer\n",
    "            for data, labels in val_loader:\n",
    "                # Transfer Data to GPU if available\n",
    "                if torch.cuda.is_available():\n",
    "                    data, labels = data.cuda(), labels.cuda()\n",
    "\n",
    "                # Forward Pass\n",
    "                target = model(data)\n",
    "                # Find the Loss\n",
    "                loss = criterion(target, labels)\n",
    "                # Calculate Loss\n",
    "                # valid_loss = loss.item() * data.size(0)\n",
    "                val_losses.update(loss.item(), data.size(0))\n",
    "\n",
    "                acc = utilities.accuracy(y_true=labels, y_pred=target)\n",
    "                # print(f\"acc{acc}\")\n",
    "                # accuracy\n",
    "                _, predicted = torch.max(target.data, 1)\n",
    "                val_accuracy.update(acc)\n",
    "                # f = f1_score(labels.cpu(), predicted.cpu(), average='micro')\n",
    "                # print(f\"f{f}\")\n",
    "                # val_f1.update(f)\n",
    "\n",
    "        data_time.update(time() - start_epoch)\n",
    "\n",
    "        print(\n",
    "            f'Epoch {e + 1} [{data_time.avg:.2f}s]: '\n",
    "            f'Training Loss: {train_losses.avg:.2f}, '\n",
    "            f'Validation Loss: {val_losses.avg:.2f}, '\n",
    "            f'Train Accuracy: {train_accuracy.avg:.2f}, '\n",
    "            f'Validation Accuracy: {val_accuracy.avg:.2f}')\n",
    "\n",
    "        # if min_valid_loss > val_losses.avg:\n",
    "        #     print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{val_losses.avg:.6f}) \\t Saving The Model')\n",
    "        #     min_valid_loss = val_losses.avg\n",
    "        #\n",
    "        #     # Saving State Dict\n",
    "        #     torch.save(model.state_dict(), cfg.DIR.BEST_MODEL + cfg.TEST.WEIGHT)\n",
    "\n",
    "    print('Finished Training')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# len(train_data_loader) * epochs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# from torchvision.models.resnet import resnet18 as _resnet18\n",
    "# import torch.optim as optim\n",
    "#\n",
    "#\n",
    "# max_lr = 0.01\n",
    "# grad_clip = 0.1\n",
    "# weight_decay = 1e-4\n",
    "#\n",
    "# model = _resnet18(pretrained=False)\n",
    "# # Set up cutom optimizer with weight decay\n",
    "# optimizer = optim.Adam(model.parameters(), max_lr, weight_decay=weight_decay)\n",
    "# # Set up one-cycle learning rate scheduler\n",
    "# sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=cfg.SOLVER.IMS_PER_BATCH,\n",
    "#                                             steps_per_epoch=len(train_data_loader))\n",
    "# criterion = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# train(cfg=cfg, model=model,\n",
    "#       optimizer=optimizer, scheduler=sched, train_loader=train_data_loader,\n",
    "#       val_loader=val_data_loader, criterion=criterion)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "from numpy import vstack\n",
    "from sklearn.metrics import accuracy_score\n",
    "# evaluate the model\n",
    "def evaluate_model(test_dl, model):\n",
    "  predictions, actuals = list(), list()\n",
    "  for i, (inputs, targets) in enumerate(test_dl):\n",
    "    if torch.cuda.is_available():\n",
    "      inputs, targets = inputs.cuda(), targets.cuda()\n",
    "    # evaluate the model on the test set\n",
    "    yhat = model(inputs)\n",
    "    # retrieve numpy array\n",
    "    yhat = yhat.detach().cpu().numpy()\n",
    "    actual = targets.cpu().numpy()\n",
    "    # convert to class labels\n",
    "    yhat = argmax(yhat, axis=1)\n",
    "    # reshape for stacking\n",
    "    actual = actual.reshape((len(actual), 1))\n",
    "    yhat = yhat.reshape((len(yhat), 1))\n",
    "    # store\n",
    "    predictions.append(yhat)\n",
    "    actuals.append(actual)\n",
    "  predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "  # calculate accuracy\n",
    "  acc = accuracy_score(actuals, predictions)\n",
    "  return acc\n",
    "\n",
    "# acc = evaluate_model(test_dl=val_data_loader,model=model)\n",
    "# print(acc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import resnet18 as _resnet18\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "max_lr = 0.001\n",
    "grad_clip = 0.1\n",
    "weight_decay = 1e-4\n",
    "epochs = 50\n",
    "model2 = _resnet18(pretrained=True)\n",
    "# Set up cutom optimizer with weight decay\n",
    "optimizer2 = optim.Adam(model2.parameters(), max_lr, weight_decay=weight_decay)\n",
    "# Set up one-cycle learning rate scheduler\n",
    "sched = torch.optim.lr_scheduler.OneCycleLR(optimizer2, max_lr, epochs=epochs,\n",
    "                                            steps_per_epoch=len(train_data_loader))\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [14.40s]: Training Loss: 7.11, Validation Loss: 5.53, Train Accuracy: 0.01, Validation Accuracy: 0.02\n",
      "Epoch 2 [14.46s]: Training Loss: 4.98, Validation Loss: 4.66, Train Accuracy: 0.02, Validation Accuracy: 0.05\n",
      "Epoch 3 [14.44s]: Training Loss: 4.59, Validation Loss: 4.11, Train Accuracy: 0.04, Validation Accuracy: 0.12\n",
      "Epoch 4 [14.60s]: Training Loss: 4.13, Validation Loss: 3.19, Train Accuracy: 0.10, Validation Accuracy: 0.23\n",
      "Epoch 5 [14.52s]: Training Loss: 3.61, Validation Loss: 2.58, Train Accuracy: 0.17, Validation Accuracy: 0.34\n",
      "Epoch 6 [14.60s]: Training Loss: 3.30, Validation Loss: 2.29, Train Accuracy: 0.22, Validation Accuracy: 0.39\n",
      "Epoch 7 [14.69s]: Training Loss: 3.15, Validation Loss: 2.25, Train Accuracy: 0.25, Validation Accuracy: 0.41\n",
      "Epoch 8 [14.74s]: Training Loss: 3.05, Validation Loss: 2.12, Train Accuracy: 0.27, Validation Accuracy: 0.44\n",
      "Epoch 9 [14.81s]: Training Loss: 2.97, Validation Loss: 2.07, Train Accuracy: 0.28, Validation Accuracy: 0.45\n",
      "Epoch 10 [14.76s]: Training Loss: 2.91, Validation Loss: 2.08, Train Accuracy: 0.30, Validation Accuracy: 0.45\n",
      "Epoch 11 [14.74s]: Training Loss: 2.85, Validation Loss: 2.08, Train Accuracy: 0.31, Validation Accuracy: 0.46\n",
      "Epoch 12 [14.71s]: Training Loss: 2.81, Validation Loss: 2.04, Train Accuracy: 0.32, Validation Accuracy: 0.46\n",
      "Epoch 13 [14.81s]: Training Loss: 2.77, Validation Loss: 1.98, Train Accuracy: 0.33, Validation Accuracy: 0.48\n",
      "Epoch 14 [14.79s]: Training Loss: 2.72, Validation Loss: 1.99, Train Accuracy: 0.34, Validation Accuracy: 0.47\n",
      "Epoch 15 [14.74s]: Training Loss: 2.67, Validation Loss: 2.01, Train Accuracy: 0.35, Validation Accuracy: 0.47\n",
      "Epoch 16 [14.80s]: Training Loss: 2.64, Validation Loss: 1.91, Train Accuracy: 0.36, Validation Accuracy: 0.50\n",
      "Epoch 17 [14.72s]: Training Loss: 2.58, Validation Loss: 1.89, Train Accuracy: 0.37, Validation Accuracy: 0.50\n",
      "Epoch 18 [14.78s]: Training Loss: 2.54, Validation Loss: 1.91, Train Accuracy: 0.38, Validation Accuracy: 0.50\n",
      "Epoch 19 [14.75s]: Training Loss: 2.51, Validation Loss: 1.93, Train Accuracy: 0.38, Validation Accuracy: 0.49\n",
      "Epoch 20 [14.72s]: Training Loss: 2.47, Validation Loss: 1.88, Train Accuracy: 0.39, Validation Accuracy: 0.51\n",
      "Epoch 21 [14.74s]: Training Loss: 2.42, Validation Loss: 1.85, Train Accuracy: 0.40, Validation Accuracy: 0.52\n",
      "Epoch 22 [14.76s]: Training Loss: 2.40, Validation Loss: 1.94, Train Accuracy: 0.41, Validation Accuracy: 0.50\n",
      "Epoch 23 [14.72s]: Training Loss: 2.35, Validation Loss: 1.88, Train Accuracy: 0.42, Validation Accuracy: 0.51\n",
      "Epoch 24 [14.79s]: Training Loss: 2.32, Validation Loss: 1.85, Train Accuracy: 0.43, Validation Accuracy: 0.52\n",
      "Epoch 25 [14.80s]: Training Loss: 2.29, Validation Loss: 1.87, Train Accuracy: 0.44, Validation Accuracy: 0.52\n",
      "Epoch 26 [14.95s]: Training Loss: 2.24, Validation Loss: 1.83, Train Accuracy: 0.45, Validation Accuracy: 0.53\n",
      "Epoch 27 [14.69s]: Training Loss: 2.20, Validation Loss: 1.84, Train Accuracy: 0.46, Validation Accuracy: 0.53\n",
      "Epoch 28 [14.78s]: Training Loss: 2.16, Validation Loss: 1.85, Train Accuracy: 0.46, Validation Accuracy: 0.53\n",
      "Epoch 29 [14.76s]: Training Loss: 2.12, Validation Loss: 1.82, Train Accuracy: 0.48, Validation Accuracy: 0.54\n",
      "Epoch 30 [14.82s]: Training Loss: 2.08, Validation Loss: 1.81, Train Accuracy: 0.48, Validation Accuracy: 0.54\n",
      "Epoch 31 [14.77s]: Training Loss: 2.05, Validation Loss: 1.80, Train Accuracy: 0.49, Validation Accuracy: 0.55\n",
      "Epoch 32 [14.81s]: Training Loss: 2.01, Validation Loss: 1.80, Train Accuracy: 0.51, Validation Accuracy: 0.55\n",
      "Epoch 33 [14.77s]: Training Loss: 1.95, Validation Loss: 1.84, Train Accuracy: 0.52, Validation Accuracy: 0.55\n",
      "Epoch 34 [15.03s]: Training Loss: 1.92, Validation Loss: 1.80, Train Accuracy: 0.53, Validation Accuracy: 0.56\n",
      "Epoch 35 [14.71s]: Training Loss: 1.86, Validation Loss: 1.88, Train Accuracy: 0.54, Validation Accuracy: 0.56\n",
      "Epoch 36 [14.73s]: Training Loss: 1.83, Validation Loss: 1.90, Train Accuracy: 0.55, Validation Accuracy: 0.56\n",
      "Epoch 37 [14.81s]: Training Loss: 1.79, Validation Loss: 1.87, Train Accuracy: 0.56, Validation Accuracy: 0.56\n",
      "Epoch 38 [14.78s]: Training Loss: 1.75, Validation Loss: 1.84, Train Accuracy: 0.57, Validation Accuracy: 0.57\n",
      "Epoch 39 [14.83s]: Training Loss: 1.73, Validation Loss: 1.89, Train Accuracy: 0.58, Validation Accuracy: 0.57\n",
      "Epoch 40 [14.81s]: Training Loss: 1.69, Validation Loss: 1.91, Train Accuracy: 0.59, Validation Accuracy: 0.57\n",
      "Epoch 41 [14.91s]: Training Loss: 1.65, Validation Loss: 1.91, Train Accuracy: 0.60, Validation Accuracy: 0.57\n",
      "Epoch 42 [14.84s]: Training Loss: 1.63, Validation Loss: 1.93, Train Accuracy: 0.60, Validation Accuracy: 0.57\n",
      "Epoch 43 [14.82s]: Training Loss: 1.59, Validation Loss: 1.94, Train Accuracy: 0.62, Validation Accuracy: 0.57\n",
      "Epoch 44 [14.79s]: Training Loss: 1.58, Validation Loss: 1.94, Train Accuracy: 0.62, Validation Accuracy: 0.57\n",
      "Epoch 45 [14.85s]: Training Loss: 1.55, Validation Loss: 1.93, Train Accuracy: 0.62, Validation Accuracy: 0.57\n",
      "Epoch 46 [14.80s]: Training Loss: 1.54, Validation Loss: 1.93, Train Accuracy: 0.63, Validation Accuracy: 0.58\n",
      "Epoch 47 [14.85s]: Training Loss: 1.54, Validation Loss: 1.95, Train Accuracy: 0.63, Validation Accuracy: 0.58\n",
      "Epoch 48 [14.87s]: Training Loss: 1.51, Validation Loss: 1.92, Train Accuracy: 0.63, Validation Accuracy: 0.58\n",
      "Epoch 49 [14.82s]: Training Loss: 1.52, Validation Loss: 1.94, Train Accuracy: 0.63, Validation Accuracy: 0.58\n",
      "Epoch 50 [14.90s]: Training Loss: 1.52, Validation Loss: 1.95, Train Accuracy: 0.63, Validation Accuracy: 0.58\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train(cfg=cfg, model=model2,\n",
    "      optimizer=optimizer2, scheduler=sched, train_loader=train_data_loader,\n",
    "      val_loader=val_data_loader, criterion=criterion,grad_clip=grad_clip)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5759\n"
     ]
    }
   ],
   "source": [
    "acc = evaluate_model(test_dl=val_data_loader,model=model2)\n",
    "print(acc)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}